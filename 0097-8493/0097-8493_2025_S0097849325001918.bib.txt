@article{WANG2025104350,
title = {PointHuman: Learning high-fidelity and generalizable human neural radiance fields using guidance of fine-grained semantics-enriched geometry},
journal = {Computers & Graphics},
volume = {131},
pages = {104350},
year = {2025},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2025.104350},
url = {https://www.sciencedirect.com/science/article/pii/S0097849325001918},
author = {Jintai Wang and Pengfei Liu and Huidong Feng and Qifeng Dai and Yinglin Zheng and Ming Zeng},
keywords = {3D Clothed human reconstruction, Generalizable human NeRF},
abstract = {In this paper, we focus on the challenging task of rendering high-fidelity humans in novel views given only sparse input reference images. Current methods encounter spatial ambiguity and render image blur because of the use of coarse human priors. Some methods achieve high-fidelity rendering but are restricted by scene-specific training or demand dense input frames. To address these limitations, we propose PointHuman, a generalizable NeRF that integrates fine-grained and semantics-enriched geometry as human priors. Our key contributions include: (1) Proposing a novel deformation field to model human 3D motion, which maps each pixel on the image to its corresponding 3D position, thereby obtaining fine-grained geometry. (2) Introducing a spatial encoding method based on the extracted geometry, which encodes each sample point as a feature aggregation of its neural point neighborhood, effectively reducing spatial ambiguity and enhancing high-fidelity rendering. Extensive experiments on several datasets demonstrate the superiority of our method over other counterparts.}
}