@article{ABBASS2025104170,
title = {Low-light image enhancement via improved lightweight YUV attention network},
journal = {Computers & Graphics},
volume = {127},
pages = {104170},
year = {2025},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2025.104170},
url = {https://www.sciencedirect.com/science/article/pii/S0097849325000093},
author = {Mohammed Y. Abbass and H. Kasban and Zeinab F. Elsharkawy},
keywords = {Low-light image enhancement, YUV color-space, Attention Mechanism, Uneven illumination Luminance domain, Low-light image dataset, Luminance domain},
abstract = {Deep learning approaches have notable results in the area of computer vision applications. Our paper presents improved LYT-Net, a Lightweight YUV Transformer-based models, as an innovative method to improve low-light scenes. Unlike traditional Retinex-based methods, the proposed framework utilizes the chrominance (U and V) and luminance (Y) channels in YUV color-space, mitigating the complexity between color details and light in scenes. LYT-Net provides a thorough contextual realization of the image while keeping architecture burdens low. In order to tackle the issue of weak feature generation of traditional Channel-wise Denoiser (CWD) Block, improved CWD is proposed using Triplet Attention network. Triplet Attention network is exploited to capture both dynamics and static features. Qualitative and quantitative experiments demonstrate that the proposed technique effectively addresses images with varying exposure levels and outperforms state-of-the-art techniques. Furthermore, the proposed technique shows faster computational performance compared to other Retinex-based techniques, promoting it as a suitable option for real-time computer vision topics. The source code is available at https://github.com/Mohammed-Abbass/YUV-Attention}
}