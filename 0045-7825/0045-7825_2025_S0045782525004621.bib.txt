@article{MO2025118190,
title = {Enhancing high-dimensional probabilistic model updating: A generic generative model-inspired framework with GAN-embedded implementation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {445},
pages = {118190},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.118190},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525004621},
author = {Jiang Mo and Wang-Ji Yan and Ka-Veng Yuen and Michael Beer},
keywords = {Probabilistic model updating, Probability distribution, Generative adversarial network, Gaussian mixture model, Reparameterization trick, Maximum mean discrepancy},
abstract = {Probabilistic model updating (PMU), which seeks to identify the probability distributions of model parameters by aligning model predictions with measured responses, is essential for ensuring the credibility of numerical models. However, PMU faces challenges like high computational costs from repeated solver calls and the curse of dimensionality in optimization. Driven by the intrinsic parallels between generative models and PMU, characterized by iterative processes aimed at minimizing disparities between generated and real data distributions, this study presents an innovative and generic PMU framework that emulates the core principles of generative models. Specifically, based on generative adversarial networks (GANs), the PMU-GAN is designed by integrating a probabilistic interpretable network as a generator and a learnable distance metric as a discriminator. By establishing this connection, the innovative approach offers compelling solutions for high-dimensional PMU, harnessing GANs’ strong fitting capacity, optimization prowess, and excellence in high-dimensional realms. The generator utilizes a Gaussian mixture model (GMM) for input distribution approximation and sampling, alongside a differentiable metamodel to expedite output sample generation in lieu of time-consuming solvers. Compared to conventional neural networks, the GMM simplifies and constrains the input distribution forms, facilitating improved convergence. By employing Gumbel-SoftMax and reparameterization tricks, the class probabilities and Gaussian component parameters unique to GMMs are embedded in the generator as trainable parameters, enabling gradient-based optimization and endowing the generator with interpretability. The discriminator, featuring an invertible network and maximum mean discrepancy, refines its ability to gauge distribution disparities through a learning process. Through adversarial training, both the generator’s generative power and the discriminator’s discernment capability are enhanced. The efficacy of the proposed method in high-dimensional PMU is substantiated through numerical and experimental demonstrations, showcasing its potential in advancing the field.}
}