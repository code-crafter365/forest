@article{WURZ2025117617,
title = {Inverse material design using deep reinforcement learning and homogenization},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {435},
pages = {117617},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.117617},
url = {https://www.sciencedirect.com/science/article/pii/S0045782524008715},
author = {V. Würz and C. Weißenfels},
keywords = {Inverse material design, Deep reinforcement learning, Advantage actor–critic model, Homogenization, Finite element method, Micro-structural parameter optimization},
abstract = {This study presents an approach to solving an inverse problem through the application of Deep Reinforcement Learning (DRL) coupled with homogenization. The underlying objective is to determine the micro-structural parameters of a composite material, including particle radius, Young’s moduli and Poisson’s ratios in order to achieve a specific target bulk modulus at the macro-scale using DRL. This approach is later extended to a multi-objective task that also decreases total material weight by incorporating density and volume fraction as additional parameters. Employing homogenization under Periodic Boundary Conditions (PBCs), a 3D mesh is analyzed comprising a matrix and particles to identify a Representative Volume Element (RVE), thereby reducing computational complexity and allowing efficient Finite Element Method (FEM) calculations in subsequent steps. An Advantage Actor–Critic (A2C) model is employed, using the FEM analysis as feedback, to iteratively adjust the micro-structural parameters and incrementally approach the target properties. A Genetic Algorithm (GA) is implemented to fine-tune the hyper-parameters of the neural network, enhancing the ability of the model to effectively explore different parameter combinations. While tuning of hyperparameters is conducted in 2D, findings are transferred to 3D to verify this approach in more realistic scenarios. The DRL approach is compared in a partial manner with the Bayesian optimization, a well established algorithm type in the field of inverse material design, in order to give an idea of the different application circumstances. For A2C, the development of a specific reward function enables the DRL algorithm to approach the solution consistently leading to many different solutions of the inverse problem. In addition, the search space is decreased significantly without limiting the variety of determined micro-configurations by using effective balancing of exploration and exploitation. Extensive tuning of hyperparameters enables the adjustment of the algorithm to specific desired outcomes as the increase of sample efficiency or quantity of diverse solutions. Using this hands-on learning approach can unveil innovative material configurations by exploring a broader range of design scenarios, including those that might be overlooked or deemed non-intuitive by traditional methodologies. Hence. it is positioned to establish an alternative methodology for designing novel material combinations with tailored properties.}
}