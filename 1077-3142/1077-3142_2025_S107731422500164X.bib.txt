@article{JIANG2025104441,
title = {PDCNet: A lightweight and efficient robotic grasp detection framework via Partial Convolution and knowledge distillation},
journal = {Computer Vision and Image Understanding},
volume = {259},
pages = {104441},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104441},
url = {https://www.sciencedirect.com/science/article/pii/S107731422500164X},
author = {Yanshu Jiang and Yanze Fang and Liwei Deng},
keywords = {Robotic, Grasping detection, Lightweight model, Knowledge distillation},
abstract = {Improving detection accuracy complicates robotic grasp models, which makes deploying them on resource-constrained edge AI devices more challenging. Although various lightweight strategies have been proposed, directly designing compact networks may not be optimal, as balancing accuracy and model size is challenging. This paper proposes a lightweight grasp detection framework, PDCNet. In response to this problem, we optimize the interplay between computational demands and detection performance. The method integrates Partial Convolution (PConv) for efficient feature extraction, Discrete Wavelet Transform (DWT) for enhancing frequency-domain feature representation, and a Cross-Stage Fusion (CSF) strategy for optimizing the utilization of multi-scale features. A Quality-Enhanced Huber Loss Function (Q-Huber) is also introduced to improve the network’s sensitivity to vital grasp localities. Finally, the teacher–student framework distills expertise into a compact student model. Comprehensive evaluations were conducted using the public datasets to demonstrate that PDCNet achieves detection accuracies of 98.7%, 95.8%, and 97.1% on Cornell, Jacquard and Jacquard_V2 datasets respectively, while maintaining minimal parameters and high computational efficiency. Real-world experiments on an embedded edge AI device further validate the capability of PDCNet to perform accurate grasp detection under limited computational resources.}
}