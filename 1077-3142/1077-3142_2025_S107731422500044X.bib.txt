@article{WEN2025104321,
title = {When super-resolution meets camouflaged object detection: A comparison study},
journal = {Computer Vision and Image Understanding},
volume = {253},
pages = {104321},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104321},
url = {https://www.sciencedirect.com/science/article/pii/S107731422500044X},
author = {Juan Wen and Shupeng Cheng and Weiyan Hou and Luc {Van Gool} and Radu Timofte},
keywords = {Super-resolution, Camouflaged object detection, Deep learning},
abstract = {Super-resolution (SR) and camouflage object detection (COD) are two prominent topics in the field of computer vision, with various joint applications. However, in previous work, these two areas were often studied in isolation. In this paper, we conduct a comprehensive comparative evaluation of both for the first time. Specifically, we benchmark different super-resolution methods on commonly used COD datasets while also evaluating the robustness of different COD models using COD data processed by SR methods. Experiments reveal challenges in preserving semantic information due to differences in targets and features between the two domains. COD relies on extracting semantic information from low-resolution images to identify camouflage targets. There is a risk of losing or distorting important semantic details during the application of SR techniques. Balancing the enhancement of spatial resolution with the preservation of semantic information is crucial for maintaining the accuracy of COD algorithms. Therefore, we propose a new SR model called Dilated Super-resolution (DISR) to enhance SR performance on COD, achieving state-of-the-art results on five commonly used SR datasets. The Urban100 x4 dataset task improved by 0.38Â dB. Using low-resolution images processed by DISR for COD tasks can enhance target visibility and significantly improve the performance of COD tasks. Our goal is to leverage the synergies between these two domains, draw insights from the complementarity of techniques in both fields, and provide insights and inspiration for future research in both communities.}
}