@article{PAN2025104442,
title = {FedVLP: Visual-aware latent prompt generation for Multimodal Federated Learning},
journal = {Computer Vision and Image Understanding},
volume = {259},
pages = {104442},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104442},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001651},
author = {Hao Pan and Xiaoli Zhao and Yuchen Jiang and Lipeng He and Bingquan Wang and Yincan Shu},
keywords = {Multimodal Federated Learning, Visual-language model, Prompt learning, Few-shot},
abstract = {Recent studies indicate that prompt learning based on CLIP-like models excels in a variety of image recognition and detection tasks, consequently, it has been applied in Multimodal Federated Learning (MMFL). Federated Prompt Learning (FPL), as a technical branch of MMFL, enables clients and servers to exchange prompts rather than model parameters during communication to address challenges such as data heterogeneity and high training costs. Many existing FPL methods rely heavily on pre-trained visual-language models, making it difficult for them to handle new and real specialized domain data. To further boost the generalization ability of FPL without compromising the personalization of clients, we propose a novel framework that generates prompts guided by visual semantics to better handle specialized and small-scale data. In our approach, each client generates visual-aware latent prompts using a Fusion Encoder and an IE-Module, enabling the learning of fine-grained knowledge. Through federated computation, clients collaboratively maintain a global prompt, allowing the learning of coarse-grained knowledge. FedVLP removes the dependency on manually designed prompt templates and demonstrates superior performance across seven datasets, including CIFAR-10, CIFAR-100, Caltech-101, FLIndustry-100, and others.}
}