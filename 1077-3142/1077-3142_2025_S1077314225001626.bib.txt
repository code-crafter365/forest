@article{SHAO2025104439,
title = {AdaptDiff: Adaptive diffusion learning for low-light image enhancement},
journal = {Computer Vision and Image Understanding},
volume = {259},
pages = {104439},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104439},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001626},
author = {Xiaotao Shao and Guipeng Zhang and Yan Shen and Boyu Zhang and Zhongli Wang and Yanlong Sun},
keywords = {Low-light image enhancement, Diffusion model, Exposure correction, Image processing},
abstract = {Recovering details obscured by noise from low-light images is a challenging task. Recent diffusion models have achieved relatively promising results in low-level vision tasks. However, there are still two issues: (1) under non-uniform illumination conditions, the low-light image cannot be restored with high quality, and (2) the models have limited generalization capabilities. To solve these problems, this paper proposes an Adaptive Enhancement Algorithm guided by a Multi-scale Structural Diffusion (AdaptDiff). AdaptDiff employs adaptive high-order mapping curves (AHMC) for pixel-by-pixel mapping of the image during the diffusion process, thereby adjusting the brightness levels between different regions within the image. In addition, a multi-scale structural guidance approach (MSGD) is proposed as an implicit bias, informing the intermediate layers of the model about the structural characteristics of the image, facilitating more effective restoration of clear images. Guiding the diffusion direction through structural information is conducive to maintaining good performance of the model even when faced with data that it has not previously encountered. Extensive experiments on popular benchmarks show that AdaptDiff achieves superior performance and efficiency.}
}