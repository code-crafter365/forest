@article{PARK2025104259,
title = {Full-body virtual try-on using top and bottom garments with wearing style control},
journal = {Computer Vision and Image Understanding},
volume = {251},
pages = {104259},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2024.104259},
url = {https://www.sciencedirect.com/science/article/pii/S1077314224003400},
author = {Soonchan Park and Jinah Park},
keywords = {Image-based virtual try-on, Image synthesis, Benchmark dataset},
abstract = {Various studies have been proposed to synthesize realistic images for image-based virtual try-on, but most of them are limited to replacing a single item on a given model, without considering wearing styles. In this paper, we address the novel problem of full-body virtual try-on with multiple garments by introducing a new benchmark dataset and an image synthesis method. Our Fashion-TB dataset provides comprehensive clothing information by mapping fashion models to their corresponding top and bottom garments, along with semantic region annotations to represent the structure of the garments. WGF-VITON, the single-stage network we have developed, generates full-body try-on images using top and bottom garments simultaneously. Instead of relying on preceding networks to estimate intermediate knowledge, modules for garment transformation and image synthesis are integrated and trained through end-to-end learning. Furthermore, our method proposes Wearing-guide scheme to control the wearing styles in the synthesized try-on images. Through various experiments, for the full-body virtual try-on task, WGF-VITON outperforms state-of-the-art networks in both quantitative and qualitative evaluations with an optimized number of parameters while allowing users to control the wearing styles of the output images. The code and data are available at https://github.com/soonchanpark/WGF-VITON.}
}