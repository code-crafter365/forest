@article{XUE2025104409,
title = {HCTD: A CNN-transformer hybrid for precise object detection in UAV aerial imagery},
journal = {Computer Vision and Image Understanding},
volume = {259},
pages = {104409},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104409},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001328},
author = {Hongcheng Xue and Zhan Tang and Yuantian Xia and Longhe Wang and Lin Li},
keywords = {UAV aerial imagery, Small object detection, Feature filtering, Conv additive self-attention, Multi-scale feature fusion},
abstract = {Object detection in UAV imagery poses substantial challenges due to severe object scale variation, dense distributions of small objects, complex backgrounds, and arbitrary orientations. These factors, compounded by high inter-class similarity and large intra-class variation caused by multi-scale targets, occlusion, and environmental interference, make aerial object detection fundamentally different from conventional scenes. Existing methods often struggle to capture global semantic information effectively and tend to overlook critical issues such as feature loss during downsampling, information redundancy, and inconsistency in cross-level feature interactions. To address these limitations, this paper proposes a hybrid CNN-Transformer-based detector, termed HCTD, specifically designed for UAV image analysis. The proposed framework integrates three novel modules: (1) a Feature Filtering Module (FFM) that enhances discriminative responses and suppresses background noise through dual global pooling (max and average) strategies; (2) a Convolutional Additive Self-attention Feature Interaction (CASFI) module that replaces dot-product attention with a lightweight additive fusion of spatial and channel interactions, enabling efficient global context modeling at reduced computational cost; and (3) a Global Context Flow Feature Pyramid Network (GC2FPN) that facilitates multi-scale semantic propagation and alignment to improve small-object detection robustness. Extensive experiments on the VisDrone2019 dataset demonstrate that HCTD-R18 and HCTD-R50 achieve 38.2%/43.7% AP50, 23.1%/24.6% AP75, and 13.9%/14.7% APS respectively. Additionally, the TIDE toolkit is employed to analyze the absolute and relative contributions of six error types, providing deeper insight into the effectiveness of each module and offering valuable guidance for future improvements. The code is available at: https://github.com/Mundane-X/HCTD.}
}