@article{SINCAN2025104498,
title = {Gloss-free Sign Language Translation: An unbiased evaluation of progress in the field},
journal = {Computer Vision and Image Understanding},
volume = {261},
pages = {104498},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104498},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225002218},
author = {Ozge Mercanoglu Sincan and Jian He Low and Sobhan Asasi and Richard Bowden},
keywords = {Sign language translation, Gloss-free, Assistive technology, Codebase},
abstract = {Sign Language Translation (SLT) aims to automatically convert visual sign language videos into spoken language text and vice versa. While recent years have seen rapid progress, the true sources of performance improvements often remain unclear. Do reported performance gains come from methodological novelty, or from the choice of a different backbone, training optimizations, hyperparameter tuning, or even differences in the calculation of evaluation metrics? This paper presents a comprehensive study of recent gloss-free SLT models by re-implementing key contributions in a unified codebase. We ensure fair comparison by standardizing preprocessing, video encoders, and training setups across all methods. Our analysis shows that many of the performance gains reported in the literature often diminish when models are evaluated under consistent conditions, suggesting that implementation details and evaluation setups play a significant role in determining results. We make the codebase publicly available here11https://github.com/ozgemercanoglu/sltbaselines. to support transparency and reproducibility in SLT research.}
}