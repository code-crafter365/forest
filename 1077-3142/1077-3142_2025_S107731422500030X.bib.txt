@article{LIN2025104307,
title = {Semantic-preserved point-based human avatar},
journal = {Computer Vision and Image Understanding},
volume = {252},
pages = {104307},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104307},
url = {https://www.sciencedirect.com/science/article/pii/S107731422500030X},
author = {Lixiang Lin and Jianke Zhu},
keywords = {Semantic preserved, Human avatar},
abstract = {To enable realistic experience in AR/VR and digital entertainment, we present the first point-based human avatar model that embodies the entirety expressive range of digital humans. Specifically, we employ two MLPs to model pose-dependent deformation and linear skinning (LBS) weights. The representation of appearance relies on a decoder and the features attached to each point. In contrast to alternative implicit approaches, the oriented points representation not only provides a more intuitive way to model human avatar animation but also significantly reduces the computational time on both training and inference. Moreover, we propose a novel method to transfer semantic information from the SMPL-X model to the points, which enables to better understand human body movements. By leveraging the semantic information of points, we can facilitate virtual try-on and human avatar composition through exchanging the points of same category across different subjects. Experimental results demonstrate the efficacy of our presented method. Our implementation is publicly available at https://github.com/l1346792580123/spa.}
}