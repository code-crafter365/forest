@article{YANG2025104390,
title = {Efficient human–object-interaction (EHOI) detection via interaction label coding and Conditional Decision},
journal = {Computer Vision and Image Understanding},
volume = {258},
pages = {104390},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104390},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001134},
author = {Tsung-Shan Yang and Yun-Cheng Wang and Chengwei Wei and Suya You and C.-C. Jay Kuo},
keywords = {Human–object interaction (HOI) detection, Error correction code, Green learning, Image understanding},
abstract = {Human–Object Interaction (HOI) detection is a fundamental task in image understanding. While deep-learning-based HOI methods provide high performance in terms of mean Average Precision (mAP), they are computationally expensive and opaque in training and inference processes. An Efficient HOI (EHOI) detector is proposed in this work to strike a good balance between detection performance, inference complexity, and mathematical transparency. EHOI is a two-stage method. In the first stage, it leverages a frozen object detector to localize the objects and extract various features as intermediate outputs. In the second stage, the first-stage outputs predict the interaction type using the XGBoost classifier. Our contributions include the application of error correction codes (ECCs) to encode rare interaction cases, which reduces the model size and the complexity of the XGBoost classifier in the second stage. Additionally, we provide a mathematical formulation of the relabeling and decision-making process. Apart from the architecture, we present qualitative results to explain the functionalities of the feedforward modules. Experimental results demonstrate the advantages of ECC-coded interaction labels and the excellent balance of detection performance and complexity of the proposed EHOI method. The codes are available: https://github.com/keevin60907/EHOI---Efficient-Human-Object-Interaction-Detector.}
}