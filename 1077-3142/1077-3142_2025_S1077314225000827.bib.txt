@article{ZHOU2025104359,
title = {Extensions in channel and class dimensions for attention-based knowledge distillation},
journal = {Computer Vision and Image Understanding},
volume = {257},
pages = {104359},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104359},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225000827},
author = {Liangtai Zhou and Weiwei Zhang and Banghui Zhang and Yufeng Guo and Junhuang Wang and Xiaobin Li and Jianqing Zhu},
keywords = {Knowledge distillation, Deep learning, Model compression},
abstract = {As knowledge distillation technology evolves, it has bifurcated into three distinct methodologies: logic-based, feature-based, and attention-based knowledge distillation. Although the principle of attention-based knowledge distillation is more intuitive, its performance lags behind the other two methods. To address this, we systematically analyze the advantages and limitations of traditional attention-based methods. In order to optimize these limitations and explore more effective attention information, we expand attention-based knowledge distillation in the channel and class dimensions, proposing Spatial Attention-based Knowledge Distillation with Channel Attention (SAKD-Channel) and Spatial Attention-based Knowledge Distillation with Class Attention (SAKD-Class). On CIFAR-100, with ResNet8Ã—4 as the student model, SAKD-Channel improves Top-1 validation accuracy by 1.98%, and SAKD-Class improves it by 3.35% compared to traditional distillation methods. On ImageNet, using ResNet18, these two methods improve Top-1 validation accuracy by 0.55% and 0.17%, respectively, over traditional methods. We also conduct extensive experiments to investigate the working mechanisms and application conditions of channel and class dimensions knowledge distillation, providing new theoretical insights for attention-based knowledge transfer.}
}