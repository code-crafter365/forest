@article{BAI2025104425,
title = {Improving a segment anything model for segmenting low-quality medical images via an adapter},
journal = {Computer Vision and Image Understanding},
volume = {259},
pages = {104425},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104425},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001481},
author = {Can Bai and Jie Wang and Xianjun Han and Zijian Wu},
keywords = {Low-quality images, Medical image segmentation, Segmentation foundation models, Medical image processing},
abstract = {With the increase in large models, segmentation foundation models have greatly improved the segmentation results of medical images. However, these foundation models often yield unsatisfactory segmentation results because their training data rarely involve low-quality images. In medical imaging, issues such as considerable noise or poor image resolution are common due to imaging equipment. Using these segmentation foundation models on such images produces poor results. To address this challenge, we utilize a low-quality perception adapter to improve the capabilities of segmentation foundation models, specifically in terms of handling low-quality medical images. First, the low-quality perception adapter distills the intrinsic statistical features from images compromised by noise or reduced clarity. These intrinsic features are aligned with textual-level attributes by employing contrastive learning. Then, we use a text-vision progressive fusion strategy, starting with multilevel textâ€“image fusion to incorporate multimodal information. Next, we incorporate visual features from the underlying segmentation foundation model. Finally, a carefully designed decoder predicts the segmented mask. The low-quality perception adapter reduces the impacts of blur and noise on the developed model, while text-based contrastive learning, along with multimodal fusion, bridge the semantic gap. Experiments demonstrate that the proposed model significantly improves segmentation accuracy on noisy or blurry medical images, with gains up to 24.6% in mIoU and 13.6% in pixel accuracy over state-of-the-art methods across multiple datasets.}
}