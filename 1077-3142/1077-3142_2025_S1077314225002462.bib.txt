@article{ZHANG2025104523,
title = {IP-CAM: Class activation mapping based on importance weights and principal-component weights for better and simpler visual explanations},
journal = {Computer Vision and Image Understanding},
volume = {261},
pages = {104523},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104523},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225002462},
author = {Wenyi Zhang and Haoran Zhang and Xisheng Zhang and Xiaohua Shen and Lejun Zou},
keywords = {Class activation map, Importance weight, Principal component analysis, Visual explanation, Explainable artificial intelligence},
abstract = {Visual explanations of deep neural networks (DNNs) have gained considerable importance in deep learning due to the lack of interpretability, which constrains human trust in DNNs. This paper proposes a new gradient-free class activation map (CAM) architecture called importance principal-component CAM (IP-CAM). The architecture not only improves the prediction accuracy of networks but also provides simpler and more reliable visual explanations. It adds importance weight layers before the classifier and assigns an importance weight to each activation map. After fine-tuning, it selects images with the highest prediction score for each class, performs principal component analysis (PCA) on activation maps of all channels, and regards the eigenvector of the first principal component as principal-component weights for that class. The final saliency map is obtained by linearly combining the activation maps, importance weights and principal-component weights. IP-CAM is evaluated on the ILSVRC 2012 dataset and RSD46-WHU dataset, whose results show that IP-CAM performs better than most previous CAM variants in recognition and localization tasks. Finally, the method is applied as a tool for interpretability, and the results illustrate that IP-CAM effectively unveils the decision-making process of DNNs through saliency maps.}
}