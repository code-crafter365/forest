@article{ZHANG2025104256,
title = {DA2: Distribution-agnostic adaptive feature adaptation for one-class classification},
journal = {Computer Vision and Image Understanding},
volume = {251},
pages = {104256},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2024.104256},
url = {https://www.sciencedirect.com/science/article/pii/S1077314224003370},
author = {Zilong Zhang and Zhibin Zhao and Xingwu Zhang and Xuefeng Chen},
keywords = {Self-supervised learning, Non-contrastive learning, One-class classification},
abstract = {One-class classification (OCC), i.e., identifying whether an example belongs to the same distribution as the training data, is essential for deploying machine learning models in the real world. Adapting the pre-trained features on the target dataset has proven to be a promising paradigm for improving OCC performance. Existing methods are constrained by assumptions about the training distribution. This contradicts the real scenario where the data distribution is unknown. In this work, we propose a simple distribution-agnostic adaptive feature adaptation method (DA2). The core idea is to adaptively cluster the features of every class tighter depending on the property of the data. We rely on the prior that the augmentation distributions of intra-class samples overlap, then align the features of different augmentations of every sample by a non-contrastive method. We find that training a random initialized predictor degrades the pre-trained backbone in the non-contrastive method. To tackle this problem, we design a learnable symmetric predictor and initialize it based on the eigenspace alignment theory. Benchmarks, the proposed challenging near-distribution experiments substantiate the capability of our method in various data distributions. Furthermore, we find that utilizing DA2 can immensely mitigate the long-standing catastrophic forgetting in feature adaptation of OCC. Code will be released upon acceptance.}
}