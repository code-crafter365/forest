@article{ZHANG2025104467,
title = {Representation learning of point cloud upsampling in global and local inputs},
journal = {Computer Vision and Image Understanding},
volume = {260},
pages = {104467},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104467},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225001900},
author = {Tongxu Zhang and Bei Wang},
keywords = {Deep learning, Point cloud, Upsampling, Feature extract, Interpretability},
abstract = {In recent years, point cloud upsampling has been widely applied in tasks such as 3D reconstruction and object recognition. This study proposed a novel framework, ReLPU, which enhances upsampling performance by explicitly learning from both global and local structural features of point clouds. Specifically, we extracted global features from uniformly segmented inputs (Average Segments) and local features from patch-based inputs of the same point cloud. These two types of features were processed through parallel autoencoders, fused, and then fed into a shared decoder for upsampling. This dual-input design improved feature completeness and cross-scale consistency, especially in sparse and noisy regions. Our framework was applied to several state-of-the-art autoencoder-based networks and validated on standard datasets. Experimental results demonstrated consistent improvements in geometric fidelity and robustness. In addition, saliency maps confirmed that parallel global-local learning significantly enhanced the interpretability and performance of point cloud upsampling.}
}