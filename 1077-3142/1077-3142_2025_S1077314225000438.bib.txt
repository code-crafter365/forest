@article{MOUAWAD2025104320,
title = {View-to-label: Multi-view consistency for self-supervised monocular 3D object detection},
journal = {Computer Vision and Image Understanding},
volume = {254},
pages = {104320},
year = {2025},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2025.104320},
url = {https://www.sciencedirect.com/science/article/pii/S1077314225000438},
author = {Issa Mouawad and Nikolas Brasch and Fabian Manhardt and Federico Tombari and Francesca Odone},
keywords = {3D object detection, Self-supervised learning, Multi-view optimization, Automotive application, Monocular system},
abstract = {For autonomous vehicles, driving safely is highly dependent on the capability to correctly perceive the environment in the 3D space, hence the task of 3D object detection represents a fundamental aspect of perception. While 3D sensors deliver accurate metric perception, monocular approaches enjoy cost and availability advantages that are valuable in a wide range of applications. Unfortunately, training monocular methods requires a vast amount of annotated data. To compensate for this need, we propose a novel approach to self-supervise 3D object detection purely from RGB video sequences, leveraging geometric constraints and weak labels. Unlike other approaches that exploit additional sensors during training, our method relies on the temporal continuity of video sequences. A supervised pre-training on synthetic data produces initial plausible 3D boxes, then our geometric and photometrically grounded losses provide a strong self-supervision signal that allows the model to be fine-tuned on real data without labels. Our experiments on Autonomous Driving benchmark datasets showcase the effectiveness and generality of our approach and the competitive performance compared to other self-supervised approaches.}
}