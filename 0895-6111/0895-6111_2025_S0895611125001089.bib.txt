@article{ZHANG2025102599,
title = {Adaptive batch-fusion self-supervised learning for ultrasound image pretraining},
journal = {Computerized Medical Imaging and Graphics},
volume = {124},
pages = {102599},
year = {2025},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2025.102599},
url = {https://www.sciencedirect.com/science/article/pii/S0895611125001089},
author = {Jiansong Zhang and Xiuming Wu and Shunlan Liu and Yuling Fan and Yongjian Chen and Guorong Lyu and Peizhong Liu and Zhonghua Liu and Shaozheng He},
keywords = {Self-supervised pretraining, Medical self-supervised learning, Ultrasound representation learning, Ultrasound image classification/segmentation},
abstract = {Medical self-supervised learning eliminates the reliance on labels, making feature extraction simple and efficient. The intricate design of pretext tasks in single-modal self-supervised analysis presents challenges, however, compounded by an excessive dependency on data augmentation, leading to a bottleneck in medical self-supervised learning research. Consequently, this paper reanalyzes the feature learnability introduced by data augmentation strategies in medical image self-supervised learning. We introduce an adaptive self-supervised learning data augmentation method from the perspective of batch fusion. Moreover, we propose a conv embedding block for learning the incremental representation between these batches. We tested 5 fused data tasks proposed by previous researchers and it achieved a linear classification protocol accuracy of 94.25% with only 150 self-supervised feature training in Vision Transformer(ViT), which is the best among the same methods. With a detailed ablation study on previous augmentation strategies, the results indicate that the proposed medical data augmentation strategy in this paper effectively represents ultrasound data features in the self-supervised learning process. The code and weights could be found at here.}
}